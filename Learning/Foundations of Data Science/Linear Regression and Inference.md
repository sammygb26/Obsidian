# Linear Regression and Inference
This will follow on from the original [[Foundations of Data Science/Linear Regression]] topic. Where $$\hat\beta_0=\bar y-\hat\beta_1\bar x$$ and $$\hat\beta_1=\frac{\sum_{i=1}^{n}(x_i-\bar x)(y_i-\bar y)}{\sum_{i=1}^{n}(x_i-\bar x)^2}$$ The derivations are in the [[Foundations of Data Science/Linear Regression]] topic. $\hat\beta_1$ and $\hat\beta_0$ are both *point estimators* so have a distribution. So we can use them for hypothesis testing and other inference. 

![[Pasted image 20220301101554.png]]

We can in fact use the [[Bootstrap Method]] to find a distribution for these estimators. Where we use the different points as the collection we sample from. If we have a boot strap distribution for $\beta_1$ and $\beta_0$ we can use them to find confidence intervals for individual predictions. We do this by *repeating* for each *bootstrap value* giving us a new distribution combining the distribution for the individual distribution for $\beta_0$ and $\beta_1$.

![[Pasted image 20220301101537.png]]

## Beyond Bootstrap
We can use the [[Bootstrap Method]] to find distribution for $\beta_0$ and $\beta_1$ but we can also find expressions for them algebraically. The standard deviation will be the standard deviation in the residuals after linear regression. That is $$s^2=\frac{1}{n-2}\sum_{i=1}^n(y_i-\hat y_i)^2=\frac{SSE}{n-2}$$ so then the estimator for the variance in $\hat\beta_1$ will be $$\hat\sigma_{\hat\beta_1}=s_{\hat\beta_1}=\frac{s}{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2}}$$ This can be translated as the more spread out and more values there are in the $x$ axis. Then the more spread out the $y$ values are the creator the $SD$ of $\hat\beta_1$. We basically have two *random variables* here that is $\hat\beta_1$ and $S_{\hat\beta_1}$ (standard error in the estimator). This therefore gives us a **T-Distribution** with $n-1$ degrees of freedom. $$T=\frac{\hat\beta_1-\beta_1}{S_{\hat\beta_1}}$$Therefore we can get the **T-critical values** for our desired confidence interval giving us our confidence interval for $1-\alpha$ as $$P\left(-t_{\frac{\alpha}{2},n-2}<\frac{\hat\beta_1-\beta_1}{S_{\hat\beta_1}}<t_{\frac{\alpha}{2},n-2}\right)=1-\alpha$$ We an rearrange the probability statement to get the $CI$ as $$\left(\beta_1-t_{\frac{\alpha}{2},n-2}\cdot S_{\hat\beta_1},\beta_1+t_{\frac{\alpha}{2},n-2}\cdot S_{\hat\beta_1}\right)$$
We can plug any value we want to test into the *t-distribution* giving us a probability mass more extreme a *p-value* hence we can easily perform hypothesis testing.

## Derivation
We will node derive the above formulae. We can start by changing our model a bit to be *probabilistic* instead of full *deterministic*. So instead we write $$y_i=\beta_0+\beta_1x_i+\epsilon_i$$ Here $\epsilon_i$ is a normally distributed random variable $$\epsilon_i\sim N(0,\sigma^2)$$ It represents the residual. Here  as in [[Foundations of Data Science/Linear Regression]] $$\beta_1=\frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum(x_i-\bar x)^2}$$Then we can say that all the residuals are equal the epsilons so $y_i-\bar y=\epsilon_i=\epsilon_i-\bar\epsilon$ this is since $E(\epsilon_i)=0$ as states in its normal distribution. Hence we get $E(y_i-\bar y) = E(\epsilon_i-\bar\epsilon_i)=0$. The variance is slightly different from the variance in $\epsilon_i$ so we get $$V(y_i-\bar y)=V(\epsilon_i-\bar\epsilon_i)=\frac{\sigma^2n}{n-1}$$The difference come from the difference in **degrees of freedom**. We want the variance of $\hat\beta_1$, we use the formula $V(\sum c_ix_i)=\sum c_i^2V(x_i)$ then we get the following $$V[\beta_1]=V\left[\frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum(x_i-\bar x)^2}\right]=\frac{\sum\left((x_i-\bar x)^2V[y_i-\bar y]\right)}{(\sum(x_i-\bar x)^2)^2}=\frac{\frac{\sigma^2n}{n-1}}{\sum(x_i-\bar x)^2}$$Then we know $s^2=\frac{\sigma^2n}{n-1}$ hence we get the following as above. $$\sigma_{\hat\beta_1}=\sqrt{\frac{\frac{\sigma^2n}{n-1}}{\sum(x_i-\bar x)^2}}=\frac{s}{\sqrt{\sum(x_i-\bar x)^2}}$$

## Principle of Least Squares
We will explain where difference in squares comes from here. We are now thinking about the *probabilistic mode* for linear regression. That is $$y_i=\beta_0+\beta_1x_i+\epsilon_i$$ Epsilon will be some random variable and we want to fit $\beta_0$ and $\beta_1$ such that our given results are the most likely. That is we use the *principle of maximum likelihood*. That is maximize $P(y_1,..., y_n; x_1,...x_n|\beta_0, \beta_1)$. The probability of a given $\epsilon_i$ will be $$P(\epsilon_i)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\epsilon_i^2/2\sigma^2}=\frac{1}{\sqrt{2\pi}\sigma}\textrm{exp}\left(-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}\right)$$ We can so say that the probability of one point will be $$P(y_i;x_i|\beta_0,\beta_1)=\frac{1}{\sqrt{2\pi}\sigma}\textrm{exp}\left(-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}\right)$$ We assume that the points are generated *independently* and that the variance is the same for all points. Therefore $$P(y_1,.., y_n; x_1,...x_n|\beta_0, \beta_1)=P(y_1;x_1|\beta_0,\beta_1)\cdot ...P(y_n;x_n|\beta_0,\beta_1)$$ Hence we can write this as $$P(y_1,..., y_n; x_1,...x_n|\beta_0, \beta_1)=\prod_{i=1}^nP(y_i;x_i|\beta_0,\beta_1)$$ Which is our formula for likelihood. We can also use log likelihood giving us the following. $$ln(P(y_1,...y_n;x_1...x_n|\beta_0,\beta_1)=ln\left(\prod_{i=1}^nP(y_i;x_i|\beta_0,\beta_1)\right)$$$$ln(P(y_1,...y_n;x_1...x_n|\beta_0,\beta_1)=\sum_{i=1}^nln\left(P(y_i;x_i|\beta_0,\beta_1)\right)$$ Now we just need the log of the individual likelihood that is $$ln\left(P(y_i;x_i|\beta_0,\beta_1)\right)=ln\left(\frac{1}{\sqrt{2\pi}\sigma}\textrm{exp}\left(-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}\right)\right)$$$$ln\left(P(y_i;x_i|\beta_0,\beta_1)\right)=-ln\left(\sqrt{2\pi}\sigma\right)-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}$$
Hence we have $$ln(P(y_1,...y_n;x_1...x_n|\beta_0,\beta_1)=\sum_{i=1}^n\left(-ln\left(\sqrt{2\pi}\sigma\right)-\frac{(y_i-\beta_0-\beta_1x_i)^2}{2\sigma^2}\right)$$$$ln(P(y_1,...y_n;x_1...x_n|\beta_0,\beta_1)=-n\cdot ln\left(\sqrt{2\pi}\sigma\right)-\frac{1}{2}\sigma^2\sum_{i=1}^n(y_i-\beta_0-\beta_1x_i)^2$$ Now $-n\cdot ln\left(\sqrt{2\pi}\sigma\right)$ and $\frac{1}{n}\sigma^2$  are both constant so to maximize the likelihood we really want to minimize our sum. Hence giving us the *principle of least squares* as the sum is the same as that form least squares. We can also differentiate and estimate $\sigma$ which will be found to be $$\hat\sigma^2=\frac{1}{n}\sum_{i=1}^n(y_i-(\beta_0 +\beta_1x_i))^2=\frac{SSE}{n}$$ which is different from out result earlier. But this comes from *max likelihood* and the other comes from *sampling theory*. The previous ones is *unbiased* while this ones is, but it does matter for anything but small $n$.

[[Linear Regression and Inference Questions]]