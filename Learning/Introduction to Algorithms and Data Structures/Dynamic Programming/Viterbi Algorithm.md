# The Viterbi Algorithm

## Hidden Markov Models (HMMs)
We are the **transducer** meaning we **generate strings**. The outputs happens at the states so the transitions are form one state to another with a probability. The transitions are now just (q, q') with a probability $p_{q,q'}$. Each state would have a probability of each character it can **output**. An example would be weather
![[Pasted image 20220131132506.png]]
Note the outgoing probabilities sum to 1 and the output symbol probabilities sum to 1. We store the probabilities of going to every state as a probability matrix ${QXQ}$. So for above Q={q1, q2, q3}. A question we might have is *what is the most likely path that produces this output sequence*. We want the **best explanation**.

For a path we can find the probability of generating a path as the product of all the probabilities of generating the *emissions* in each state along with the probabilities of generating each path.

## Computing "max likelihood"
We want the most likely path through our $M$ to give our string $s$. We know it has to end somewhere, although it could be multiple. So instead of most likely path we want most likely path that ends at some $q$. We define
$$
\textrm{mlp}[s,q]=\textrm{highest probability of generating s ending at q}
$$
We want most likely path for $s=s_1, ... s_m$ ending in $q$. Process ended at $q$ hence $s_m$ was generated by $b_q$ (this is conditional on us ending at $q$). We know $s_1...s_{m-1}$ will have been generated first (ending somewhere) with some probability. Then there was a transition from **somewhere** to $q$ with probability $P[?,q]$ then times $b_q$. So we know the probability of ending at $q$ from some $q^*$ will be the probability from $q$ to $q^*$ time the probability of generating $s_m$. So chance for it in fact being $q^*$ will be
$$
\textrm{mlp}[s_1...s_{m-1}, q^*]\cdot \textrm{P}[p^*,p]\cdot b_{q,s_m}
$$
So this gives a recurrence relation
![[Pasted image 20220131135331.png]]
Here $mlp$ is a array storing the $mlp$ values where the first dimension is the part of the string. We would also have a pre which stores the previous state maximizing likelihood. This gives the algorithm
![[Pasted image 20220131135621.png]]

#### Analysis
Since there are two for loops over Q with another loop with over S we will have $\Theta(n\cdot|Q|)$ time. A problem is as we are multiplying probabilities we can get very small inaccurate numbers so log odds can be used instead.

[[Probabilistic FMSs and the Viterbi Algorithm Questions]]



