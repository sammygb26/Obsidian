Here we want to compare the performance of two systems. We may have two systems and want to compare them on a metric like $F_1$. We are comparing two classifiers $A$ and $B$ on a test set $x$, then $M(A,x)$ is the score $A$ gets on $x$. Then $\delta(x)$ is the performance difference between $A$ and $B$. $$\delta(x)=M(A,x)-M(B,x)$$We want to know if $\delta(x)>0$ that is $A$ is better than $B$. $\delta(x)$ is called the **effect size** larger $\delta(x)$ would mean $A$ is "more better" than $B$. We cannot check if $\delta(x)$ is positive or negative as values close to 0 may be dependent on noise and so wont give a good idea of which model is better. What we want to know instead is would $A$ be better consistently as we use different $x'$ for example. In **statistical hypothesis testing** we formalize two hypotheses $$H_0:\delta(x)\le 0 \hspace{64pt}H_1:\delta(x)>0$$$H_0$ is called the **null hypothesis**. We would like to know if we can confidently rule out this hypothesis. This is done by creating a random variable $X$ ranging over all test sets. We then ask how likely it is that the null hypothesis is true that is $H_0$ is correct. We can look at the likelihood that our $\delta(x)$ value is found and call this the **p-value**. $$P(\delta(X)\ge\delta(x)\mid H_0 \text{ is \textbf{true}})$$A very small p-value means the result we saw was very unlikely given the null hypothesis. This can allow us to safely reject the null hypothesis. If the p-value is smaller than $0.01$ we say $H_0$ is false and the result "$A$ is better than $B$" is **statistically significant**. Now we need to compute this value. We can use **parametric** and **non-parametric** approaches to do this. Generally NLP uses **parametric** approaches as they are easier to calculate. In **non-parametric** we sample a distribution to gain a approximate probability probability of what we see given our null hypothesis.

There are two common non-parametric tests used in NLP: **approximate randomization** and the **bootstrap test**. **Paired** test are those in which we compare two sets of observations that are aligned. This is natural when comparing models since the models both create some metric for a given test set. We pair the two performances on the same set.

### The Paired Bootstrap Test
This can be applied to any metric (precision, recall, F1, BLEU etc). In **bootstrapping** we repeatedly draw large number of samples with replacement (called bootstrap samples) from an original set. We are creating many virtual tests from an observed set by sampling from it.

![[Pasted image 20230408122535.png]]

For example above $x$ is our original set of 10 tests, the subsequent $x^{(i)}$ are synthetic samples. They are created by randomly taking 10 from the original set. But with replacement so element 2 can be taken multiple times for example. This is done many times say $2^5$. Now that we have a large number we can treat it as a distribution and perform statistic on it. We can compute a p-value by going over many test sets and counting the percentage of times $H_0$ is true. Below with $\mathbb 1$ meaning 1 if the statement is true and 0 if not. We want to know how "surprising" $\delta(x)$ is so we look are how often we get a more extreme value. This gives us:$$\text{p-value}=\frac1b\sum_{i=1}^b\mathbb1\left(\delta(x^{(i)}-\delta(x)\ge0\right)$$Now our overall set is biased (has non 0 mean). But we are assuming $A$ is a good as $B$ and so the $\delta(x)$ mean value should be taken away to get an accurate measure. This gives $$\text{p-value}=\frac1b\sum_{i=1}^b\mathbb1\left(\delta(x^{(i)}-2\delta(x)\ge0\right)$$The full algorithm is given as

![[Pasted image 20230408123900.png]]

[[Methods in Annotation and Evaluation]]