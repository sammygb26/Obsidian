There are many harms that can arise from classifiers interacting with *language* as language can be used to represent and group. Through language classification take place over groups. For example **representational harms** can be seen when sentiment analysis is performed on text is common African-American names as compared to common White-American names. In other tasks like *toxicity detection* common detector overclassify word containing phrases like "women", "blind people" or "gay people" or even special forms of language. This can lead to unintended censorship. These biases can come from training data bias or even model architecture bias. For this reasons it is important to document how a model is trained. This is done with a **model card** for each version of a model. This will document information like: 

- training algorithms and parameters
- training data sources, motivation and preprocessing
- evaluation data sources, motivation and preprocessing
- intended use and users
- model performance across different demographic or other groups and environmental situations

[[Methods in Annotation and Evaluation]]
