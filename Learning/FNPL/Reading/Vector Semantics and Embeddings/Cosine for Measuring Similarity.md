We can measure the similarity of two vectors with a **metric**. The two vector must be other the same length. One of the most common ways to do this is with the **cosine** of the angle between the vectors. This is based on the **dot product**. $$\mathbf v\cdot\mathbf w=\sum_{i=1}^Nv_iw_i$$This is a good similarity measure as if two vectors have large values in the same spaces the result will be larger than if they were in different places. But the problem with this is **long** vectors will be similar to more things and will always have larger scores. Where the length is $$|\mathbf v|=\sqrt{\sum_{i=1}^Nv_i^2}$$More common words will be longer as they co-occur with more words. We can normalize the dot product by dividing by the lengths of the two vectors. This will give use the cosine of the angle between them. $$\cos\theta=\frac{\mathbf a\cdot\mathbf b}{|\mathbf a||\mathbf b|}$$The final **cosine** similarity matric will be $$\text{cosine}(\mathbf v,\mathbf w)=\frac{\sum_{i=1}^Nv_iw_i}{\sqrt{\sum_{i=1}^Nv_i^2\sum_{i=1}^Nw_i^2}}$$It is often easier to pre-normalize the vectors so their length is one. This will give **unit vectors**. Usually cosine varies between 1 and -1 with -1 for vectors in the opposite direction. But co-occurrence values are always positive so the value varies between 0 and 1. We can n ow compare the angles to understand word similarity.

![[Pasted image 20230411163117.png]]
