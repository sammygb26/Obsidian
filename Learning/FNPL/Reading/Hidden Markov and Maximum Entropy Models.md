**Markov chains** are a extension of FSAs. A **weighted finite state automate** is the simplest extension. Each arc will have a probability assigned to it with the total probability moving out from a vertex equaling one. A **Markov chain** is a special case of this in which the input sequence inequity defines the state sequence. Now it can only give a probability to a sequence of states rather than parse any hidden information.

![[Pasted image 20230417182455.png]]

The second model here (b) represents a **bigram** language model. The model will be define by 

- $Q$ - A set of $N$ states
- $A$ - a **transition probability matrix** with $a_{ij}$ as the probability of moving from $i$ to $j$. So it must be the case that $\sum_{j=1}^na_{ij}=1$.
- $q_0,q_F$ - the special **start state** and **end state** which don't take observations.

The **Markov Chain** has an important assumption that the current state only depends on the previous state. Stated as $$\textbf{Markov Assumption:}\hspace{32pt}P(q_i\mid q_1\dots q_{i-1})=P(q_i\mid q_{i-1})$$Then since probability must sum to one for all possible next state the probabilities our of a state must sum to 1! We can also remove the start and end state and instead have a distribution over the first state. This will be represented with $\pi_1\dots \pi_N$ where $\sum_{i=1}^N\pi_i=1$.

## Hidden Markov Model
The basic idea here is we want to model a sequence of **hidden states** which are tied probabilistically to the FSA states. Formally we have 

- $Q$ - the set of $N$ states
- $A$ - A **Transition probability matrix**. Again with the condition that $\sum_{j=1}^na_{ij}=1$ $\forall i$ 
- $O$ - A sequence of $T$ **observations**
- $B$ - A sequence of **observation likelihoods** for the probability of some observation $o_t$ being generated in state $i$.
- $q_o,q_F$ - a special **start state** and **final state** along with their transition probabilities $a_{01},a_{02},\dots,a_{0N}$ and $a_{1F},a_{2F},\dots,a_{NF}$.

The final two may also be seperated for $\pi$ initial state probabilities and $q_x,q_y,\dots q_z$ accepting states. There are two assumptions, that the next state only depends on the current state **Markov assumption** and that the observation is conditionally independent of the other observations given its state. $$\textbf{Output Assumption: }P(o_i\mid q_1,\dots,q_i,\dots,q_T,o_1,\dots,o_T)=P(o_i\mid q_i)$$
## Likelihood Computation : The Forward Algorithm
Here we want to know, what is the likelihood of a given sequence. For example $313$. That is given a HMM $\lambda=(A,B)$ and an observation sequence $O$, determine the likelihood $P(O\mid \lambda)$. We know if we know the hidden states $Q$ then the probability would be $$P(O\mid Q)=\prod_{i=1}^TP(o_i\mid q_i)$$Now we could enumerate over all possible state and this way get a probability, but of course that would be expensive. We can instead keep track of the cumulative probability we have ended up in a given state with the preceding observations. If we knew after $t$ observations what $P(q_i | O_{1:t})$ was then the probability after the next observation for any state from $q_i$ will be  $$P(q_i\mid O_{1:t})=P(o_{t+1}\mid q_i)\sum_{q'\in Q}P(q'\mid O_{1:t})P(q_i\mid q')$$That is the probability for any state will be the probability we ended up in that state (from summed from all possible states in the previous timestep) times the probability that state gave the observation we saw. The likelihood in the end will be the sum of all the likelihoods in the final state. The initial values $P(q_i)=\pi_i$.

## Decoding: The Viterbi Algorithm
Here we want to **decode** and get the most likely sequence of state given our observations. Again there is a dynamic programming solution. To see why we can think about the properties of the most likely sequence. If it goes through at state $q$ in time $t$. Then the path from the start to $q$ at time $t$ must be the most likely path otherwise there would be another more likely path we could have taken. So for any path instate time step $t$ we need only keep track of the highest probability that could have reached that path. We can do this for every timestep and the trellis we will build up will allow us to backtrack from the end, once we have reached our most likely state.

So we can define $v_{t-1}(i)$ to be the best likelihood for state $i$ in time $t-1$. Then $a_{ij}$ is the transition probability from state $i$ to $j$. Finally $b_j(o_t)$ is the observation likelihood for the next observation. Then $v_t(j)$ will be $$v_t(j)=\max_{q_i\in Q}\left(v_{t-1}(i)\cdot a_{ij}\cdot b_j(o_t)\right)$$In the end we pick the final state which has the largest likelihood in state $v_T$. The **initialization** takes $v_0(i)=\pi_i$.

## Training: The Forward-Backward Algorithm
First we can define the **backward probability** which is similar to the *forward probability*. We define 

- $\beta_t(i)$ - The probability of seeing the observations from tie $t+1$ to the end given we are in state $i$ at time $t$.

Similarly to the forward algorithm we can compute this in a dynamic way by noting that if we know the $\beta_{t+1}$ values, $A$ and $B$ then we can find $\beta_t$ by summing up all the probabilities weighted by the chance the state make emission $o_{t+1}$ after transitioning to $i$. That is $$\beta_t(i)=\sum_{q_j\in Q}a_{ij}\cdot b_j(o_{t+1})\cdot \beta_{t+1}(j)$$Where the initial values will be $\beta_T(i)=a_{iF}$. Now the **FB algorithm** proceeds in a similar way to [[Expectation Maximization]]. We notes we can estimate $\hat a_{ij}$ by noting that $$a_{ij}=\frac{\text{expected number of transitions from state $i$ to state $j$}}{\text{expected number if transition from state $i$}}$$Now how can we calculate the **numerator**. We can think about what this would be for a single timestep. We call this $\xi_t$  and it can be calculated as$$\xi_t(i,j)=P(q_t=i,q_{t+1}=j\mid O,\lambda)$$Now we can calculate something similar
$$P(q_t=i,q_{t+1}=j,O\mid\lambda)=\alpha_{t}(i)\cdot a_{ij}\cdot b_j(o_{t+1})\cdot\beta_{t+1}(j)$$This forward probability is $\alpha_t(i)=P(O_{1:t},q_t=i\mid\lambda)$ and so includes the $O$ observations as does the backward probability $\beta_{t+1}(j)=P(O_{t+2:T}\mid q_{t+1}=j,\lambda)$. Now we can add in $a_{ij}$ and $b_j(o_t)$ to get $$\alpha_i(t)\cdot a_{ij}=P(O_{1:t},q_t=i,q_{t+1}=j)\mid\lambda)$$$$b_j(t)\cdot\beta_{t+1}(j)=P(O_{t+1:T}\mid q_{t+1}=j,\lambda)$$This will cancel out to give the above equation. Now we can divide the whole thing by $P(O|\lambda)$ to basically normalize by the chance of the whole sequence giving us $$\xi_t(i,j)=\frac{\alpha_t(i)\cdot a_{ij}\cdot b_t(o_{t+1})\beta_{{t+1}}(j)}{\alpha_T(N)}$$Then we can integrate over all times and divide by the bottom possibility for all outgoing states to get $$\hat a_{ij}=\frac{\sum_{t=1}^T\xi_{t}(i,j)}{\sum_{t=1}^T\sum_{j=1}^N\xi_t(i,j)}$$Now we just need an estimate for $\hat b_j(v_k)$ which will be $$\hat b_k(v_k)=\frac{\text{expected number of time in state $j$ and observing symbol }v_k}{\text{expected number of times in state }j}$$We can use the same trick and find out how often we would be in state $j$ observing $v_k$ at time $t$. We can calculate $\gamma_t(j)$ as the change of being in state $j$ at time $t$. This will be $$\gamma_t(j)=P(q_t=j\mid O,\lambda)=\frac{P(q=j, O\mid \lambda)}{P(O\mid \lambda)}$$$$=\frac{P(O_{1:t},q_t=j\mid\lambda)P(O_{t+1:T}\mid q_t=j,\lambda)}{P(O\mid\lambda)}=\frac{\alpha_t(j)\cdot\beta_t(j)}{\alpha_T(N)}$$Now the data we want to move this value over is the true observations so which will give us the cumulative probability of being in state $j$ while $v_k$ is observed over the total probability of being in state $j$. That is $$\hat b_j(v_k)=\frac{\sum_{t=1\text{ s.t.}o_t=v_k}^T\gamma_t(j)}{\sum_{t=1}^T\gamma_t(j)}$$Now to run the algorithm we alternatively calculate $\gamma$ and $\xi$ then update the estimates for $A$ and $B$. The cool thing is here it is as if we were using many simulated hidden states for $\gamma$ and $\xi$ but the dynamic approach makes it quite efficient.