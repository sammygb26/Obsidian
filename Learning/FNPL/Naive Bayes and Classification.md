This is a basic approach to **classification**. We will apply this to **sentiment analysis**. The simplest version of this is +/- (good or bad) classification. Certain words may give informative clues.

![[Pasted image 20230202120158.png]]

As a form of classification there are many techniques. However **supervised machine learning** is a good option as it doesn't rely on brittle human crafted rules. Here we want to match a **document** or **feature** ($x$/$d$) to a class ($c$/$y$).

Some other forms of text classification are **spam detection** and **topic assignment** (sport/finance/travel/etc). For *sentiment* we can also define different sentiment. For example what is the text's sentiment towards different things. 

We can also try to classify things about the *author* of th text. For example **native language identification**, **diagnosis of disease** and **identification of gender**, **dialect** or **educational background**. Of course this can come with *ethical issues*.

### N-gram models for Classification
N-gram models can sometimes be used for classification. This can work for identifying features at a local level within the language (like for identifying native author language). The problem is for many task, sequential relationships between words are largely irrelevant. This is the level analysis n-grams work on (n-grams of words). Hence they bay not give the best results. 

We may also want to include other kinds of features like POS tags which n-grams don't include. Hence we will consider other types of classifiers.

## Naive Bayes Classifiers
Here we make the assumption that the order of the words doesn't matter, only the number of words.

![[Pasted image 20230202164540.png]]

The naive bayes is a probabilistic classifier meaning for a document $d$ out of all classes $c\in C$ the classifier returns the class $\hat c$ which has the maximum posterior probability given the document. $$\hat c=\underset{c\in C}{\text{argmax}}P(c\mid d)$$This is called **Bayesian inference**. We apply bayes rule to transform the above equation to get one we can work with. Giving $$\hat c=\underset{c\in C}{\text{argmax}}\frac{P(d\mid c)P(c)}{P(d)}
=\underset{c\in C}{\text{argmax}}P(d\mid c)P(c)$$As $P(d)$ is constant with respect to $c$. This is a **generator** as we start with an implicit assumption about how a document is generated: a class is sampled from $P(c)$, and then the words are generated by sampling from $P(d\mid c$). This is done the same way some data could be generated. First we "generate" a class $c$ ($P(c)$) and then we use this to "generate" our document $d$.

- $P(c)$ is the prior
- $P(d\mid c)$ is the

### How to model $P(d\mid c)$
We define a set of **feature** that might help classify the document. We'll assume these are all words, but we could just use **some** words (e.g. removing stop words). But can use other info like POS tags etc.

This is still hard to calculate as we would need to know the probability of any given $d$ for any class $c$. Hence we make BOW assumption and **naive bayes** assuming all words are independent of each other. This gives

![[Pasted image 20230203122901.png]]

This assumption is crazy hence the **naive** in the name. So we only care about the counts. Our data can then be represented as (in BOW):

![[Pasted image 20230203122823.png]]

This can be used to give:

![[Pasted image 20230202165404.png]]

Now e only need word probabilities give a class. In reality log space is used but it is equivalent to multiplying probabilities. This is called **costs** ($-\log p(x)$)

### Training Naive Bayes Classifier
We need to calculate $P(c)$ and $P(f_i\mid c)$. For $P(c)$ we can use MLE

![[Pasted image 20230202170653.png]]

Then $P(f_i\mid c)$ is the same again just we will take $f_i$ to be some word in the BOW.

![[Pasted image 20230202170804.png]]

A problem with this is if some word never appears in our training example. say fantastic is positive example then we will get

![[Pasted image 20230202170927.png]]

All probabilities are multiplied together so this will give $0$ no matter the other values. For this we need [[Evaluation and Smoothing]] (smoothing). With **Laplace** smoothing we get

![[Pasted image 20230202171107.png]]

We can also use a smaller constant than $1$ but it doesn't matter as much to use more power full smoothing techniques as precise probabilities are less important than the overall size:

![[Pasted image 20230203123153.png]]

We will also remove **unknown words** and **stop words** as they wont add semantically meaningful information.

![[Pasted image 20230202171321.png]]

### Worked Example
Here will will use naive Bayes with add-one smoothing (Laplace). Here are the miniature training and test documents

![[Pasted image 20230202171555.png]]

$P(C)$ the prior can be calculated as

![[Pasted image 20230202171625.png]]

We remove all unseen words to simplify the naive bayes. We then calculate the likelihoods as

![[Pasted image 20230202171753.png]]

For the test sentence we have:

![[Pasted image 20230202171820.png]]

### Costs and linearity
Using costs, our Naive bayes equation looks like

![[Pasted image 20230203123834.png]]

This amount so a sum of feature value sin linear space. Hence this can be called a **linear classifier**.


### Optimizing for Sentiment Analysis
Generally if a word appears or not is more important than how many times. So word counts can be clipped at 1 *per document*. This gives us **binary multinomial naive Bayes** or **binary naive Bayes**.  An example is given as

![[Pasted image 20230202172129.png]]

**Negation** - This can flip the semantics of certain words "I did like" vs "I didn't like". One way to deal with this is to add NOT_ as a prefix to negated words. Hence

![[Pasted image 20230202172309.png]]

**Sentiment Lexicons** - Here if we don't have enough training data we can use predefined list of how + or - words are.

![[Pasted image 20230202172453.png]]

**Ignore Stop Words**: We can remove stopwords as they don't give much information in a BAW context.

### Limited Annotated Data
In practive **annotated texts are often hard to obtain**. We may have lots of unlabeled data and little labeled data. So we would like to incorporate unannotated texts when estimating NB? For this we can use a **semi-supervised** approach. Take the set of labeled and unlabeled data (in practice with much more labels).

![[Pasted image 20230203124301.png]]

The word **Bayes** can be seen in the labeled and unlabeled data. We have a model of the other words. We can look at the association with other words. One idea is **self-training**. We train first labeled data alone. We then predict labels on unlabeled data. Then we re-estimate NB incorporating self-labeled data.

![[Pasted image 20230203124632.png]]

But we can make **mistakes** and then train from these predictions. This can cause the probabilities to stray from the probabilities we would like. But if we look at the mistake there was really no *data* to go off. The model should have been **unconfident**. So we shoudn't trust these predicted labels the same as our gold labels. We label it wiht a score (fractional label).

![[Pasted image 20230203124936.png]]

##### EM for Semi-supervised Learning
![[Pasted image 20230203125049.png]]

We can redefine this as a case of expectation maximization. It can be show to maximize likelihood of observed data. **EM** is very general and comes up a lot (Variational Autoencoder for deep learning for example). This is a form of **hard EM** however.

## Advantages of Naive Bayes
This is easy to implement. It's also fast to train and classify new documents. Doesn't require as much training data as some other methods. Usually words reasonably well. This is a baseline algorithm.

### Disadvantages of Naive Bayes
The assumption is very naive and gives poor results. Since structure is ignored the probabilities aren't well dialed in. Only roughly correct. Many features may also not be related to the category.

### Evaluation: Precision, Recall, F-measure
If we have predicted a sentiment for a class we want to measure how often and when we are correct. We will do this by comparing to **gold labels**.

![[Pasted image 20230202173458.png]]

**Accuracy** is good but doesn't work if the labels are unbalanced. Hence we used **prevision**: what percentage of our positive guesses for a class are correct and **recall**: what percentage of some class did we get correct. 

We can use the **F-measure** to combine these two values. It is defined as $$F_\beta=\frac{(\beta^2+1)PR}{\beta^2P+R}$$$\beta$ differentially weights the importance of recall and precision. So small beta values *precision* less and recalled more (visa vera for $\beta>1$). With $\beta=1$ they are weight evenly giving $F_1$ as:

![[Pasted image 20230202174104.png]]

This is known as the harmonic means of precision and recall.

![[Pasted image 20230202174847.png]]

This gives our F-measure as

![[Pasted image 20230202174913.png]]

This *weights the lower of $P$ and $R$ more* as it is the harmonic mean.

##### Evaluating with more than two classes
We often need more classes (usually even sentiment analysis needs three). Naive bayes works on multiple classes but for our evaluation we will have to expand.

![[Pasted image 20230202175121.png]]

This is the **confusion matrix** for more classes. We can calculate precision and recall for each class. This is called **macro averaging** as we have all classes together. 

![[Pasted image 20230202175637.png]]

We can also perform **micro-averaging** were averaging is performed on individual classes. This ensure smallest classes aren't washed out.

## Test Sets and Cross-validation
Generally we use a **training set** to get the model low level parameters like $P(c)$ and $P(w\mid c)$. Then a **dev-set** is used to train hyperparameters. Then we also need a **training-set** for our final evaluation. The problem is we may have to use lots of our data just for these extra sets we don't train on.

We can use **cross-validation** to overcome this. Here we partition the data into $k$ disjoin subsets called **folds**. For each of these folds we train our classifier on all but one of the folds and perform dev testing on the remainder. We do this for each of the $k$ folds. The only problem with this is we technically can't look at the data as it is all test data.

![[Pasted image 20230202180139.png]]

## Statistical Significance Testing
We need to perform this in order to know if a system is truly better than some existing one. If we have two systems $A$ and $B$. We may want to compare some score $M$ on some data $x$ between $A$ and $B$. To see which is better this gives

![[Pasted image 20230202180311.png]]

$\delta(x)$ is called th **effect size**. The issue is for small values we may not be sure if $A$ is better than $B$. In truth $\delta(x)$ is a random variable and has a distribution. We want to be sure the true performance is better or worse. Or how likely it is to be better or worse.

In **statistical hypothesis testing** we give two hypothesis

![[Pasted image 20230202180515.png]]

$H_0$ is the **null hypothesis** which could be $\delta(x)$ is negative and $A$ is not better than $B$. Formally we create a random variable $X$ raging over all test sets. Then we ask how likely it is that the null hypothesis is correct.

![[Pasted image 20230202180733.png]]

This is defined as out $p$-value.  With some large $\delta(x)$ meaning $A$ is much better than $B$ in our test. This will be much smaller. If this $p$-value is smaller than 0.01 we say the result is **statistically significant**.

Generally we test on lots of data to get an approximation of this $p$-value in NLP. This kind of testing is called **non-parametric**.

##### The Paired Bootstrap Test
In **bootstrapping** we repeatedly pull some value from a distribution to estimate its distribution. This can be applied to any score. We only need assume the sample is representative of the whole "population". In the example below we sample $b$ subsamples uniformly with repetition.

![[Pasted image 20230202181758.png]]

Calculating $\delta$ on each gives us a distribution. We can take the average over the samples that our null hypothesis is true to be

![[Pasted image 20230202181926.png]]

We need many test sets in order to get this correct.

![[Pasted image 20230202182131.png]]

[[Naive Bayes and Classification Questions]]