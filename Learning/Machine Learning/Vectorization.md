# Vectorization
Often we may have many for loops in our code that slow it down. **Vectorization** allows these to be removed. In [[Machine Learning/Binary Classification and Logistic Regression]] we have $z=w^Tx+b$ where $w$ and $x$ are high dimensional vectors. In an *un-vectorized* implementation we would calculate each $w_ix_i$ term individually. However in a *vectorized* we'll compute $w^Tx$ directly. In python we say $z=np.dot(w,x)+b$. This effect is even more pronounced when using a GPU. This is all done using SIMD instructions standing for Single Instruction Multiple Data.

In our implementation of Logistic Regression in [[Machine Learning/Binary Classification and Logistic Regression]] we need to compute $w^tX+b$ for our feature and example matrix $X$. Here $b$ is **broadcast** over all the rows of $w^TX$ as if it were a row vector. However it is more efficient than making a row vector say in a for loop. *Broadcasting* can also be done with vectors to matrices and matrices to higher dimensional structures.